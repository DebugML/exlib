{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import TextClassificationPipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "from transformers import Pipeline\n",
    "from torch import Tensor \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "import sentence_transformers\n",
    "\n",
    "sys.path.append(os.path.abspath('../src/exlib/utils'))\n",
    "from projection_helper import project_points_onto_axes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_REPO = \"go_emotions\"\n",
    "MODEL_REPO = \"shreyahavaldar/roberta-base-go_emotions\"\n",
    "TOKENIZER_REPO = \"roberta-base\"\n",
    "\n",
    "def load_data():\n",
    "    hf_dataset = load_dataset(DATASET_REPO)\n",
    "    return hf_dataset\n",
    "\n",
    "def load_model():\n",
    "    model = AutoModel.from_pretrained(MODEL_REPO)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "#go emotions dataset\n",
    "class EmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        dataset = load_dataset(DATASET_REPO)[split]        \n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_REPO)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        label = self.dataset[idx]['labels'][0]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label)\n",
    "        }\n",
    "\n",
    "#classifier for go emotions dataset\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(MODEL_REPO)\n",
    "        self.classifier = nn.Linear(768, 28)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        cls_token = last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_token)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample inference on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at shreyahavaldar/roberta-base-go_emotions and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/5427 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My favourite food is anything I didn't have to cook myself.\n",
      "Emotion: 15\n",
      "\n",
      "Text: Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead\n",
      "Emotion: 9\n",
      "\n",
      "Text: WHY THE FUCK IS BAYLESS ISOING\n",
      "Emotion: 6\n",
      "\n",
      "Text: To make her feel threatened\n",
      "Emotion: 14\n",
      "\n",
      "Text: Dirty Southern Wankers\n",
      "Emotion: 8\n",
      "\n",
      "Text: OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe PlAyOfFs! Dumbass Broncos fans circa December 2015.\n",
      "Emotion: 14\n",
      "\n",
      "Text: Yes I heard abt the f bombs! That has to be why. Thanks for your reply:) until then hubby and I will anxiously wait ðŸ˜\n",
      "Emotion: 12\n",
      "\n",
      "Text: We need more boards and to create a bit more space for [NAME]. Then weâ€™ll be good.\n",
      "Emotion: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = EmotionDataset(\"train\")\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "model = EmotionClassifier()\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(dataloader): \n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    utterances = [dataset.tokenizer.decode(input_id, skip_special_tokens=True) for input_id in input_ids]\n",
    "    for utterance, label in zip(utterances, output):\n",
    "        print(\"Text: {}\\nEmotion: {}\\n\".format(utterance, label.argmax()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Alignment Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(nn.Module): \n",
    "    def __init__(self, model_name:str=\"distiluse-base-multilingual-cased\"): \n",
    "        super(Metric, self).__init__()\n",
    "        self.model = sentence_transformers.SentenceTransformer(model_name)\n",
    "        points = self.define_circumplex()\n",
    "        self.x1 = points[0]\n",
    "        self.x2 = points[1]\n",
    "        self.y1 = points[3]\n",
    "        self.y2 = points[2]\n",
    "\n",
    "    def define_circumplex(self):\n",
    "        emotions = pd.read_csv(\"../src/exlib/utils/russell_emotions.csv\")\n",
    "        axis_labels = [\"NV\", \"PV\", \"HA\", \"LA\"]\n",
    "        axis_points = []\n",
    "        for label in axis_labels:\n",
    "            emotion_words = emotions[emotions[\"label\"] == label][\"emotion\"].values\n",
    "            emotion_embeddings = self.model.encode(emotion_words)\n",
    "            axis_points.append(np.mean(emotion_embeddings, axis=0))\n",
    "        return axis_points\n",
    "    \n",
    "    def distance_from_circumplex(self, embeddings):\n",
    "        projection = project_points_onto_axes(embeddings, self.x1, self.x2, self.y1, self.y2)\n",
    "        x_projections = projection[0]\n",
    "        y_projections = projection[1]\n",
    "        distances = []\n",
    "        for x, y in zip(x_projections, y_projections):\n",
    "            distances.append(np.abs(np.sqrt(x**2 + y**2)-1))\n",
    "        return np.mean(distances)\n",
    "\n",
    "    # input: list of words\n",
    "    def calculate_group_alignment(self, groups:list, language:str=\"english\"):\n",
    "        distances = []\n",
    "        for group in groups:\n",
    "            embeddings = self.model.encode(group)\n",
    "            distances.append(1 - self.distance_from_circumplex(embeddings))\n",
    "        return distances\n",
    "        \n",
    "    def forward(self, zp, x=None, y=None, z=None, reduce=True, **kwargs): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Group Alignment Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: ['however', 'therefore', 'unless'], Alignment: 0.19223788470945413\n",
      "Group: ['sad', 'happy', 'thrilled'], Alignment: 0.6713999482756581\n",
      "Group: ['computer', 'neural network', 'compiler'], Alignment: 0.3275037898438139\n",
      "Group: ['tired', 'sleepy', 'calm'], Alignment: 0.653776497435735\n"
     ]
    }
   ],
   "source": [
    "metric = Metric()\n",
    "sample_groups = [[\"however\", \"therefore\", \"unless\"], \n",
    "                [\"sad\", \"happy\", \"thrilled\"], \n",
    "                [\"computer\", \"neural network\", \"compiler\"], \n",
    "                [\"tired\", \"sleepy\", \"calm\"]]\n",
    "alignments = metric.calculate_group_alignment(sample_groups)\n",
    "for group, alignment in zip(sample_groups, alignments):\n",
    "    print(f\"Group: {group}, Alignment: {alignment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtop",
   "language": "python",
   "name": "vtop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
